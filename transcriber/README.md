# Audio Transcriber mit Sprechertrennung

Eine standalone Python-Anwendung f√ºr Audio-Transkription mit automatischer Sprechertrennung und Export-Funktionen.

## Features

- **Audio-Transkription** mit OpenAI Whisper
  - Mehrere Modellgr√∂√üen (tiny bis large)
  - Automatische Spracherkennung
  - Unterst√ºtzt viele Audioformate (MP3, WAV, M4A, FLAC, OGG, etc.)

- **Sprechertrennung** (Speaker Diarization)
  - Automatische Erkennung verschiedener Sprecher
  - Konfigurierbare Anzahl von Sprechern
  - Powered by pyannote.audio

- **Export-Funktionen**
  - PDF-Export mit formatierter Ausgabe
  - TXT-Export als Plain Text
  - Zeitstempel f√ºr alle Segmente
  - Sprecher-Labels bei aktivierter Diarization

- **Benutzerfreundliche Web-UI**
  - Moderne Gradio-Oberfl√§che
  - Upload oder Aufnahme direkt im Browser
  - Echtzeit-Vorschau der Ergebnisse

## Systemanforderungen

- Python 3.8 oder h√∂her
- FFmpeg (f√ºr Audio-Verarbeitung)
- Optional: NVIDIA GPU mit CUDA f√ºr schnellere Verarbeitung

### FFmpeg installieren

**Linux (Ubuntu/Debian):**
```bash
sudo apt-get update
sudo apt-get install ffmpeg
```

**macOS:**
```bash
brew install ffmpeg
```

**Windows:**
Laden Sie FFmpeg von https://ffmpeg.org/download.html herunter und f√ºgen Sie es zum PATH hinzu.

## Installation

### 1. Repository klonen oder Ordner herunterladen

```bash
cd transcriber
```

### 2. HuggingFace Token einrichten (f√ºr Sprechertrennung)

1. Registrieren Sie sich bei [HuggingFace](https://huggingface.co/)
2. Akzeptieren Sie die Nutzungsbedingungen f√ºr [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)
3. Erstellen Sie einen Token unter [HuggingFace Settings](https://huggingface.co/settings/tokens)
4. Kopieren Sie `.env.example` zu `.env`:
   ```bash
   cp .env.example .env
   ```
5. F√ºgen Sie Ihren Token in die `.env` Datei ein:
   ```
   HUGGINGFACE_TOKEN=your_actual_token_here
   ```

**Hinweis:** Die Sprechertrennung funktioniert nur mit einem g√ºltigen HuggingFace Token. Transkription ohne Sprechertrennung funktioniert auch ohne Token.

### 3. Anwendung starten

**Linux/macOS:**
```bash
./start.sh
```

**Windows:**
```bash
start.bat
```

Das Startscript:
- Erstellt automatisch eine virtuelle Python-Umgebung
- Installiert alle ben√∂tigten Dependencies
- Startet die Anwendung

## Verwendung

1. √ñffnen Sie einen Browser und navigieren Sie zu `http://localhost:7860`

2. **Audiodatei hochladen:**
   - Klicken Sie auf "Audiodatei" um eine Datei hochzuladen
   - Oder nutzen Sie das Mikrofon-Symbol f√ºr direkte Aufnahme

3. **Einstellungen anpassen:**
   - **Modellgr√∂√üe:** W√§hlen Sie zwischen tiny, base, small, medium, large
     - `tiny`: Schnell, weniger genau (~39M Parameter)
     - `base`: Gute Balance (~74M Parameter)
     - `small`: Bessere Qualit√§t (~244M Parameter)
     - `medium`: Sehr gute Qualit√§t (~769M Parameter)
     - `large`: Beste Qualit√§t (~1550M Parameter)

   - **Sprache:** Auto-Erkennung oder manuell ausw√§hlen

   - **Sprechertrennung:** Aktivieren f√ºr Meetings, Interviews, Podcasts
     - Geben Sie die gesch√§tzte Anzahl Sprecher an (2-10)

4. **Transkribieren:**
   - Klicken Sie auf "üéØ Transkribieren"
   - Warten Sie auf die Verarbeitung (kann je nach Modell und L√§nge variieren)

5. **Ergebnis exportieren:**
   - **PDF:** Formatiertes Dokument mit Zeitstempeln und Sprechern
   - **TXT:** Plain Text Format
   - Dateien werden automatisch zum Download bereitgestellt

## Tipps f√ºr beste Ergebnisse

### Modellwahl
- F√ºr schnelle Tests: `base`
- F√ºr Produktion: `medium` oder `large`
- Bei limitierten Ressourcen: `small`

### Audioqualit√§t
- Verwenden Sie Aufnahmen mit wenig Hintergrundger√§uschen
- Idealerweise 16kHz oder h√∂her Sample-Rate
- Mono oder Stereo werden unterst√ºtzt

### Sprechertrennung
- Funktioniert am besten bei klarer Trennung der Sprecher
- Geben Sie die korrekte Anzahl Sprecher an wenn bekannt
- Bei Unsicherheit: lassen Sie das System sch√§tzen

## Architektur

```
transcriber/
‚îú‚îÄ‚îÄ app.py              # Hauptanwendung mit Gradio UI
‚îú‚îÄ‚îÄ transcriber.py      # Whisper Transkriptions-Logik
‚îú‚îÄ‚îÄ diarization.py      # Speaker Diarization
‚îú‚îÄ‚îÄ export.py           # PDF/TXT Export
‚îú‚îÄ‚îÄ requirements.txt    # Python Dependencies
‚îú‚îÄ‚îÄ .env.example        # Beispiel-Konfiguration
‚îú‚îÄ‚îÄ start.sh           # Linux/macOS Startscript
‚îî‚îÄ‚îÄ start.bat          # Windows Startscript
```

## Technologie-Stack

- **[OpenAI Whisper](https://github.com/openai/whisper)** - State-of-the-art Spracherkennung
- **[pyannote.audio](https://github.com/pyannote/pyannote-audio)** - Speaker Diarization
- **[Gradio](https://gradio.app/)** - Web UI Framework
- **[ReportLab](https://www.reportlab.com/)** - PDF Generation
- **PyTorch** - Deep Learning Framework

## GPU-Beschleunigung

Wenn Sie eine NVIDIA GPU haben, installieren Sie PyTorch mit CUDA-Unterst√ºtzung:

```bash
# Aktiviere virtuelle Umgebung
source venv/bin/activate  # Linux/macOS
# oder
venv\Scripts\activate.bat  # Windows

# Installiere PyTorch mit CUDA (siehe https://pytorch.org)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

Die Anwendung erkennt automatisch CUDA und nutzt die GPU wenn verf√ºgbar.

## Fehlerbehebung

### "No module named 'whisper'"
```bash
source venv/bin/activate
pip install -r requirements.txt
```

### "FFmpeg not found"
Installieren Sie FFmpeg (siehe Systemanforderungen oben).

### "HuggingFace Token fehlt" bei Sprechertrennung
1. Erstellen Sie eine `.env` Datei basierend auf `.env.example`
2. F√ºgen Sie Ihren HuggingFace Token hinzu
3. Akzeptieren Sie die Nutzungsbedingungen auf HuggingFace

### Langsame Verarbeitung
- Verwenden Sie ein kleineres Modell (tiny, base, small)
- Installieren Sie CUDA-Support f√ºr GPU-Beschleunigung
- Deaktivieren Sie Sprechertrennung wenn nicht ben√∂tigt

## Performance

Gesch√§tzte Verarbeitungszeiten f√ºr 1 Minute Audio (CPU - Intel i7):

| Modell | Nur Transkription | Mit Sprechertrennung |
|--------|-------------------|---------------------|
| tiny   | ~2-3 Sekunden    | ~10-15 Sekunden     |
| base   | ~5-7 Sekunden    | ~15-20 Sekunden     |
| small  | ~15-20 Sekunden  | ~30-40 Sekunden     |
| medium | ~30-45 Sekunden  | ~60-90 Sekunden     |
| large  | ~60-90 Sekunden  | ~2-3 Minuten        |

Mit GPU k√∂nnen diese Zeiten um 5-10x reduziert werden.

## Sicherheit

- Die Anwendung l√§uft standardm√§√üig nur auf localhost
- Keine Daten werden an externe Server gesendet (au√üer Modell-Downloads)
- Audiodateien werden tempor√§r verarbeitet und k√∂nnen gel√∂scht werden
- HuggingFace Token sollte niemals √∂ffentlich geteilt werden

## Lizenz

Bitte beachten Sie die Lizenzen der verwendeten Bibliotheken:
- OpenAI Whisper: MIT License
- pyannote.audio: MIT License
- Gradio: Apache License 2.0

## Support

Bei Problemen oder Fragen:
1. √úberpr√ºfen Sie die Fehlerbehebung oben
2. Pr√ºfen Sie die Logs in der Konsole
3. Stellen Sie sicher, dass alle Systemanforderungen erf√ºllt sind

## Weiterentwicklung

M√∂gliche Erweiterungen:
- Batch-Verarbeitung mehrerer Dateien
- Cloud-Storage-Integration
- Mehrsprachige UI
- Live-Transkription von Streams
- Weitere Export-Formate (DOCX, SRT f√ºr Untertitel)
- REST API f√ºr Integration in andere Systeme
